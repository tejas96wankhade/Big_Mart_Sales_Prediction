{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BigMart Sales Prediction - Comprehensive Modeling Pipeline\n",
    "# Progressive complexity approach: Linear Models ‚Üí Tree Models ‚Üí Advanced Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Tree-based Models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "# Advanced Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATA LOADING AND PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"Load preprocessed data\"\"\"\n",
    "    print(\"Loading processed data...\")\n",
    "    \n",
    "    # Load processed training data\n",
    "    train_df = pd.read_csv('data/processed/train_processed.csv')\n",
    "    \n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Columns: {len(train_df.columns)}\")\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "def prepare_modeling_data(df, target_col='Item_Outlet_Sales'):\n",
    "    \"\"\"Prepare data for modeling\"\"\"\n",
    "    print(\"Preparing data for modeling...\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in ['Item_Identifier', 'Outlet_Identifier', target_col]]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "    print(f\"Target variable: {target_col}\")\n",
    "    print(f\"Feature columns: {feature_cols}\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Load data\n",
    "train_df = load_processed_data()\n",
    "X, y, feature_cols = prepare_modeling_data(train_df)\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(y.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 2. FEATURE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def perform_feature_selection(X, y, k=15, method='f_regression'):\n",
    "    \"\"\"Perform statistical feature selection\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FEATURE SELECTION (SelectKBest, k={k})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Apply SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    feature_scores = selector.scores_[selector.get_support()]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'F_Score': feature_scores\n",
    "    }).sort_values('F_Score', ascending=False)\n",
    "    \n",
    "    print(\"Selected Features (ranked by F-score):\")\n",
    "    print(feature_importance_df.to_string(index=False))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['F_Score'])\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'])\n",
    "    plt.xlabel('F-Score')\n",
    "    plt.title(f'Top {k} Features by F-Score')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X_selected, selector, selected_features\n",
    "\n",
    "# Perform feature selection\n",
    "X_selected, feature_selector, selected_features = perform_feature_selection(X, y, k=15)\n",
    "\n",
    "print(f\"\\nSelected features shape: {X_selected.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. MODEL EVALUATION FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation framework\"\"\"\n",
    "    \n",
    "    def __init__(self, cv_folds=5, random_state=42):\n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, model, X, y, model_name, use_scaling=False):\n",
    "        \"\"\"Evaluate model using cross-validation\"\"\"\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_eval = X.copy()\n",
    "        if use_scaling:\n",
    "            scaler = StandardScaler()\n",
    "            X_eval = scaler.fit_transform(X_eval)\n",
    "        \n",
    "        # Cross-validation setup\n",
    "        kfold = KFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # Store fold results\n",
    "        rmse_scores = []\n",
    "        mae_scores = []\n",
    "        r2_scores = []\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_eval), 1):\n",
    "            X_train_fold = X_eval[train_idx]\n",
    "            X_val_fold = X_eval[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            y_val_fold = y.iloc[val_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "            mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "            r2 = r2_score(y_val_fold, y_pred)\n",
    "            \n",
    "            rmse_scores.append(rmse)\n",
    "            mae_scores.append(mae)\n",
    "            r2_scores.append(r2)\n",
    "            \n",
    "            print(f\"  Fold {fold}: RMSE={rmse:.4f}, MAE={mae:.4f}, R¬≤={r2:.4f}\")\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'rmse_mean': np.mean(rmse_scores),\n",
    "            'rmse_std': np.std(rmse_scores),\n",
    "            'mae_mean': np.mean(mae_scores),\n",
    "            'mae_std': np.std(mae_scores),\n",
    "            'r2_mean': np.mean(r2_scores),\n",
    "            'r2_std': np.std(r2_scores),\n",
    "            'use_scaling': use_scaling\n",
    "        }\n",
    "        \n",
    "        self.results[model_name] = results\n",
    "        \n",
    "        print(f\"  Summary: RMSE={results['rmse_mean']:.4f}¬±{results['rmse_std']:.4f}, \"\n",
    "              f\"R¬≤={results['r2_mean']:.4f}¬±{results['r2_std']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_results_summary(self):\n",
    "        \"\"\"Get summary of all model results\"\"\"\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        results_df = pd.DataFrame(self.results).T\n",
    "        results_df = results_df.sort_values('rmse_mean')\n",
    "        \n",
    "        # Format results for display\n",
    "        results_df['CV_RMSE'] = (results_df['rmse_mean'].round(2).astype(str) + \n",
    "                                ' ¬± ' + results_df['rmse_std'].round(2).astype(str))\n",
    "        results_df['CV_R2'] = (results_df['r2_mean'].round(4).astype(str) + \n",
    "                              ' ¬± ' + results_df['r2_std'].round(4).astype(str))\n",
    "        \n",
    "        return results_df[['CV_RMSE', 'CV_R2', 'use_scaling']]\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot comparison of model results\"\"\"\n",
    "        if not self.results:\n",
    "            return\n",
    "        \n",
    "        results_df = pd.DataFrame(self.results).T\n",
    "        results_df = results_df.sort_values('rmse_mean')\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # RMSE comparison\n",
    "        axes[0].barh(range(len(results_df)), results_df['rmse_mean'])\n",
    "        axes[0].set_yticks(range(len(results_df)))\n",
    "        axes[0].set_yticklabels(results_df.index)\n",
    "        axes[0].set_xlabel('RMSE')\n",
    "        axes[0].set_title('Model Comparison - RMSE')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # R¬≤ comparison\n",
    "        axes[1].barh(range(len(results_df)), results_df['r2_mean'])\n",
    "        axes[1].set_yticks(range(len(results_df)))\n",
    "        axes[1].set_yticklabels(results_df.index)\n",
    "        axes[1].set_xlabel('R¬≤ Score')\n",
    "        axes[1].set_title('Model Comparison - R¬≤')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # RMSE with error bars\n",
    "        axes[2].errorbar(results_df['rmse_mean'], range(len(results_df)), \n",
    "                        xerr=results_df['rmse_std'], fmt='o', capsize=5)\n",
    "        axes[2].set_yticks(range(len(results_df)))\n",
    "        axes[2].set_yticklabels(results_df.index)\n",
    "        axes[2].set_xlabel('RMSE')\n",
    "        axes[2].set_title('Model Comparison - RMSE with Std Dev')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(cv_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. PHASE 1: BASELINE LINEAR MODELS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PHASE 1: BASELINE LINEAR MODELS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 1. Multiple Linear Regression\n",
    "print(\"\\n1. Multiple Linear Regression\")\n",
    "lr_model = LinearRegression()\n",
    "evaluator.evaluate_model(lr_model, X_selected, y, \"Linear_Regression\", use_scaling=True)\n",
    "\n",
    "# 2. Ridge Regression with hyperparameter tuning\n",
    "print(\"\\n2. Ridge Regression (with hyperparameter tuning)\")\n",
    "\n",
    "# Grid search for Ridge\n",
    "ridge_params = {'alpha': [0.1, 1.0, 10.0, 50.0, 100.0, 200.0, 500.0]}\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=3, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(StandardScaler().fit_transform(X_selected), y)\n",
    "\n",
    "print(f\"   Best Ridge alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "ridge_model = Ridge(alpha=ridge_grid.best_params_['alpha'])\n",
    "evaluator.evaluate_model(ridge_model, X_selected, y, \"Ridge_Regression\", use_scaling=True)\n",
    "\n",
    "# 3. Lasso Regression with hyperparameter tuning\n",
    "print(\"\\n3. Lasso Regression (with hyperparameter tuning)\")\n",
    "\n",
    "# Grid search for Lasso\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 50.0, 100.0]}\n",
    "lasso_grid = GridSearchCV(Lasso(), lasso_params, cv=3, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(StandardScaler().fit_transform(X_selected), y)\n",
    "\n",
    "print(f\"   Best Lasso alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "lasso_model = Lasso(alpha=lasso_grid.best_params_['alpha'])\n",
    "evaluator.evaluate_model(lasso_model, X_selected, y, \"Lasso_Regression\", use_scaling=True)\n",
    "\n",
    "# 4. ElasticNet for comparison\n",
    "print(\"\\n4. ElasticNet Regression (with hyperparameter tuning)\")\n",
    "\n",
    "# Grid search for ElasticNet\n",
    "elastic_params = {\n",
    "    'alpha': [0.1, 1.0, 10.0],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "}\n",
    "elastic_grid = GridSearchCV(ElasticNet(), elastic_params, cv=3, scoring='neg_mean_squared_error')\n",
    "elastic_grid.fit(StandardScaler().fit_transform(X_selected), y)\n",
    "\n",
    "print(f\"   Best ElasticNet params: {elastic_grid.best_params_}\")\n",
    "elastic_model = ElasticNet(**elastic_grid.best_params_)\n",
    "evaluator.evaluate_model(elastic_model, X_selected, y, \"ElasticNet_Regression\", use_scaling=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 5. PHASE 2: TREE-BASED MODELS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PHASE 2: TREE-BASED MODELS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 5. Decision Tree\n",
    "print(\"\\n5. Decision Tree (with hyperparameter tuning)\")\n",
    "\n",
    "# Grid search for Decision Tree\n",
    "dt_params = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "dt_grid = GridSearchCV(DecisionTreeRegressor(random_state=42), dt_params, \n",
    "                       cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "dt_grid.fit(X_selected, y)\n",
    "\n",
    "print(f\"   Best Decision Tree params: {dt_grid.best_params_}\")\n",
    "dt_model = DecisionTreeRegressor(**dt_grid.best_params_, random_state=42)\n",
    "evaluator.evaluate_model(dt_model, X_selected, y, \"Decision_Tree\")\n",
    "\n",
    "# 6. Random Forest\n",
    "print(\"\\n6. Random Forest (with hyperparameter tuning)\")\n",
    "\n",
    "# Randomized search for Random Forest (faster)\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rf_random = RandomizedSearchCV(RandomForestRegressor(random_state=42), rf_params,\n",
    "                               n_iter=20, cv=3, scoring='neg_mean_squared_error', \n",
    "                               n_jobs=-1, random_state=42)\n",
    "rf_random.fit(X_selected, y)\n",
    "\n",
    "print(f\"   Best Random Forest params: {rf_random.best_params_}\")\n",
    "rf_model = RandomForestRegressor(**rf_random.best_params_, random_state=42)\n",
    "evaluator.evaluate_model(rf_model, X_selected, y, \"Random_Forest\")\n",
    "\n",
    "# 7. Extra Trees\n",
    "print(\"\\n7. Extra Trees (with hyperparameter tuning)\")\n",
    "\n",
    "# Randomized search for Extra Trees\n",
    "et_params = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "et_random = RandomizedSearchCV(ExtraTreesRegressor(random_state=42), et_params,\n",
    "                               n_iter=20, cv=3, scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1, random_state=42)\n",
    "et_random.fit(X_selected, y)\n",
    "\n",
    "print(f\"   Best Extra Trees params: {et_random.best_params_}\")\n",
    "et_model = ExtraTreesRegressor(**et_random.best_params_, random_state=42)\n",
    "evaluator.evaluate_model(et_model, X_selected, y, \"Extra_Trees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 6. PHASE 3: ADVANCED ENSEMBLE MODELS WITH OPTUNA OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PHASE 3: ADVANCED ENSEMBLE MODELS (OPTUNA OPTIMIZATION)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "def optimize_lightgbm(X, y, n_trials=100):\n",
    "    \"\"\"Optimize LightGBM hyperparameters using Optuna\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        # 3-fold CV for faster optimization\n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        rmse_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kfold.split(X):\n",
    "            X_train_fold = X[train_idx]\n",
    "            X_val_fold = X[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            y_val_fold = y.iloc[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "            rmse_scores.append(rmse)\n",
    "        \n",
    "        return np.mean(rmse_scores)\n",
    "    \n",
    "    print(\"   Optimizing LightGBM hyperparameters...\")\n",
    "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"   LightGBM - Best RMSE: {study.best_value:.4f}\")\n",
    "    print(f\"   Best params: {study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def optimize_xgboost(X, y, n_trials=100):\n",
    "    \"\"\"Optimize XGBoost hyperparameters using Optuna\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 8),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        \n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        rmse_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kfold.split(X):\n",
    "            X_train_fold = X[train_idx]\n",
    "            X_val_fold = X[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            y_val_fold = y.iloc[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "            rmse_scores.append(rmse)\n",
    "        \n",
    "        return np.mean(rmse_scores)\n",
    "    \n",
    "    print(\"   Optimizing XGBoost hyperparameters...\")\n",
    "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"   XGBoost - Best RMSE: {study.best_value:.4f}\")\n",
    "    print(f\"   Best params: {study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def optimize_catboost(X, y, n_trials=100):\n",
    "    \"\"\"Optimize CatBoost hyperparameters using Optuna\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 500, 1500),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'depth': trial.suggest_int('depth', 3, 8),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 8.0),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.7, 1.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 50),\n",
    "            'random_state': 42,\n",
    "            'verbose': False\n",
    "        }\n",
    "        \n",
    "        model = cb.CatBoostRegressor(**params)\n",
    "        \n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        rmse_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kfold.split(X):\n",
    "            X_train_fold = X[train_idx]\n",
    "            X_val_fold = X[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            y_val_fold = y.iloc[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "            rmse_scores.append(rmse)\n",
    "        \n",
    "        return np.mean(rmse_scores)\n",
    "    \n",
    "    print(\"   Optimizing CatBoost hyperparameters...\")\n",
    "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"   CatBoost - Best RMSE: {study.best_value:.4f}\")\n",
    "    print(f\"   Best params: {study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "# 8. LightGBM\n",
    "print(\"\\n8. LightGBM (Optuna Optimization)\")\n",
    "lgb_best_params = optimize_lightgbm(X_selected, y, n_trials=100)\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_best_params)\n",
    "evaluator.evaluate_model(lgb_model, X_selected, y, \"LightGBM_Optimized\")\n",
    "\n",
    "# 9. XGBoost\n",
    "print(\"\\n9. XGBoost (Optuna Optimization)\")\n",
    "xgb_best_params = optimize_xgboost(X_selected, y, n_trials=100)\n",
    "xgb_model = xgb.XGBRegressor(**xgb_best_params)\n",
    "evaluator.evaluate_model(xgb_model, X_selected, y, \"XGBoost_Optimized\")\n",
    "\n",
    "# 10. CatBoost\n",
    "print(\"\\n10. CatBoost (Optuna Optimization)\")\n",
    "cb_best_params = optimize_catboost(X_selected, y, n_trials=100)\n",
    "cb_model = cb.CatBoostRegressor(**cb_best_params)\n",
    "evaluator.evaluate_model(cb_model, X_selected, y, \"CatBoost_Optimized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. MODEL COMPARISON AND RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL COMPARISON AND RESULTS ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Display results summary\n",
    "results_summary = evaluator.get_results_summary()\n",
    "print(\"\\nModel Performance Summary (sorted by RMSE):\")\n",
    "print(results_summary.to_string())\n",
    "\n",
    "# Plot model comparison\n",
    "evaluator.plot_results()\n",
    "\n",
    "# Identify best models\n",
    "results_df = pd.DataFrame(evaluator.results).T\n",
    "best_3_models = results_df.nsmallest(3, 'rmse_mean')\n",
    "\n",
    "print(f\"\\nTop 3 Best Performing Models:\")\n",
    "for i, (model_name, row) in enumerate(best_3_models.iterrows(), 1):\n",
    "    print(f\"{i}. {model_name}: RMSE={row['rmse_mean']:.4f}¬±{row['rmse_std']:.4f}, \"\n",
    "          f\"R¬≤={row['r2_mean']:.4f}¬±{row['r2_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 8. FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_importance_advanced(models_dict, X, feature_names):\n",
    "    \"\"\"Analyze feature importance from tree-based models\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Focus on tree-based models\n",
    "    tree_models = {\n",
    "        'Random Forest': rf_model,\n",
    "        'Extra Trees': et_model,\n",
    "        'LightGBM': lgb_model,\n",
    "        'XGBoost': xgb_model,\n",
    "        'CatBoost': cb_model\n",
    "    }\n",
    "    \n",
    "    # Train models on full data for feature importance\n",
    "    for name, model in tree_models.items():\n",
    "        if name in ['Random Forest', 'Extra Trees']:\n",
    "            model.fit(X, y)\n",
    "        else:\n",
    "            model.fit(X, y)\n",
    "    \n",
    "    # Extract feature importances\n",
    "    importance_df = pd.DataFrame({'Feature': selected_features})\n",
    "    \n",
    "    for name, model in tree_models.items():\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df[f'{name}_Importance'] = model.feature_importances_\n",
    "    \n",
    "    # Calculate average importance\n",
    "    importance_cols = [col for col in importance_df.columns if 'Importance' in col]\n",
    "    importance_df['Average_Importance'] = importance_df[importance_cols].mean(axis=1)\n",
    "    \n",
    "    # Sort by average importance\n",
    "    importance_df = importance_df.sort_values('Average_Importance', ascending=False)\n",
    "    \n",
    "    print(\"Feature Importance Summary:\")\n",
    "    print(importance_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create subplots for different models\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (name, model) in enumerate(tree_models.items()):\n",
    "        if i < len(axes) and hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:10]  # Top 10 features\n",
    "            \n",
    "            axes[i].bar(range(10), importances[indices])\n",
    "            axes[i].set_title(f'{name} - Top 10 Features')\n",
    "            axes[i].set_xticks(range(10))\n",
    "            axes[i].set_xticklabels([selected_features[idx] for idx in indices], rotation=45)\n",
    "    \n",
    "    # Average importance plot\n",
    "    if len(axes) > len(tree_models):\n",
    "        top_10_avg = importance_df.head(10)\n",
    "        axes[-1].bar(range(10), top_10_avg['Average_Importance'])\n",
    "        axes[-1].set_title('Average Importance - Top 10 Features')\n",
    "        axes[-1].set_xticks(range(10))\n",
    "        axes[-1].set_xticklabels(top_10_avg['Feature'], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance_results = analyze_feature_importance_advanced(\n",
    "    {}, X_selected, selected_features\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 9. ENSEMBLE MODEL CREATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENSEMBLE MODEL CREATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "class EnsembleModel:\n",
    "    \"\"\"Simple ensemble model using weighted averaging\"\"\"\n",
    "    \n",
    "    def __init__(self, models, weights=None):\n",
    "        self.models = models\n",
    "        self.weights = weights if weights else [1.0] * len(models)\n",
    "        self.weights = np.array(self.weights) / np.sum(self.weights)  # Normalize\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit all base models\"\"\"\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            pred = model.predict(X)\n",
    "            predictions += self.weights[i] * pred\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Select top 3 models for ensemble\n",
    "top_model_names = best_3_models.index.tolist()\n",
    "print(f\"Creating ensemble from top 3 models: {top_model_names}\")\n",
    "\n",
    "# Create ensemble models based on best performers\n",
    "ensemble_models = []\n",
    "ensemble_names = []\n",
    "\n",
    "if 'LightGBM_Optimized' in top_model_names:\n",
    "    ensemble_models.append(lgb.LGBMRegressor(**lgb_best_params))\n",
    "if 'XGBoost_Optimized' in top_model_names:\n",
    "    ensemble_models.append(xgb.XGBRegressor(**xgb_best_params))\n",
    "if 'CatBoost_Optimized' in top_model_names:\n",
    "    ensemble_models.append(cb.CatBoostRegressor(**cb_best_params))\n",
    "\n",
    "# Add best traditional model if space available\n",
    "for name in ['Random_Forest', 'Extra_Trees']:\n",
    "    if name in top_model_names and len(ensemble_models) < 3:\n",
    "        if name == 'Random_Forest':\n",
    "            ensemble_models.append(RandomForestRegressor(**rf_random.best_params_, random_state=42))\n",
    "        elif name == 'Extra_Trees':\n",
    "            ensemble_models.append(ExtraTreesRegressor(**et_random.best_params_, random_state=42))\n",
    "\n",
    "# Create and evaluate ensemble\n",
    "if len(ensemble_models) >= 2:\n",
    "    ensemble = EnsembleModel(ensemble_models)\n",
    "    evaluator.evaluate_model(ensemble, X_selected, y, \"Ensemble_Top3\")\n",
    "    \n",
    "    # Try weighted ensemble (inverse RMSE weighting)\n",
    "    rmse_values = [evaluator.results[name]['rmse_mean'] for name in top_model_names[:len(ensemble_models)]]\n",
    "    weights = [1/rmse for rmse in rmse_values]\n",
    "    \n",
    "    ensemble_weighted = EnsembleModel(ensemble_models, weights)\n",
    "    evaluator.evaluate_model(ensemble_weighted, X_selected, y, \"Ensemble_Weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 10. FINAL RESULTS AND MODEL SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESULTS AND MODEL SELECTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Final results summary\n",
    "final_results = evaluator.get_results_summary()\n",
    "print(\"Final Model Performance Summary:\")\n",
    "print(final_results.to_string())\n",
    "\n",
    "# Select best model\n",
    "all_results = pd.DataFrame(evaluator.results).T\n",
    "best_model_name = all_results['rmse_mean'].idxmin()\n",
    "best_model_rmse = all_results.loc[best_model_name, 'rmse_mean']\n",
    "best_model_r2 = all_results.loc[best_model_name, 'r2_mean']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   RMSE: {best_model_rmse:.4f}¬±{all_results.loc[best_model_name, 'rmse_std']:.4f}\")\n",
    "print(f\"   R¬≤: {best_model_r2:.4f}¬±{all_results.loc[best_model_name, 'r2_std']:.4f}\")\n",
    "\n",
    "# Performance insights\n",
    "print(f\"\\nüìä PERFORMANCE INSIGHTS:\")\n",
    "print(f\"1. Model Complexity vs Performance:\")\n",
    "\n",
    "# Group models by complexity\n",
    "linear_models = ['Linear_Regression', 'Ridge_Regression', 'Lasso_Regression', 'ElasticNet_Regression']\n",
    "tree_models = ['Decision_Tree', 'Random_Forest', 'Extra_Trees']\n",
    "advanced_models = ['LightGBM_Optimized', 'XGBoost_Optimized', 'CatBoost_Optimized']\n",
    "ensemble_models_names = [name for name in all_results.index if 'Ensemble' in name]\n",
    "\n",
    "for group_name, models in [('Linear Models', linear_models), \n",
    "                          ('Tree Models', tree_models),\n",
    "                          ('Advanced Models', advanced_models),\n",
    "                          ('Ensemble Models', ensemble_models_names)]:\n",
    "    group_results = all_results[all_results.index.isin(models)]\n",
    "    if not group_results.empty:\n",
    "        best_in_group = group_results['rmse_mean'].min()\n",
    "        print(f\"   {group_name}: Best RMSE = {best_in_group:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Key Observations:\")\n",
    "print(f\"   - Linear models baseline: RMSE ~{all_results.loc[all_results.index.isin(linear_models), 'rmse_mean'].min():.0f}\")\n",
    "print(f\"   - Tree models improvement: RMSE ~{all_results.loc[all_results.index.isin(tree_models), 'rmse_mean'].min():.0f}\")\n",
    "print(f\"   - Advanced models best: RMSE ~{all_results.loc[all_results.index.isin(advanced_models), 'rmse_mean'].min():.0f}\")\n",
    "\n",
    "if ensemble_models_names:\n",
    "    ensemble_rmse = all_results.loc[all_results.index.isin(ensemble_models_names), 'rmse_mean'].min()\n",
    "    print(f\"   - Ensemble performance: RMSE ~{ensemble_rmse:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 11. SAVE RESULTS AND TRAINED MODELS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAVING RESULTS AND MODELS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Create results directory\n",
    "import os\n",
    "os.makedirs('results/model_artifacts', exist_ok=True)\n",
    "os.makedirs('results/submissions', exist_ok=True)\n",
    "\n",
    "# Save results summary\n",
    "final_results.to_csv('results/model_performance_summary.csv')\n",
    "print(\"‚úì Model performance summary saved to: results/model_performance_summary.csv\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_results.to_csv('results/feature_importance_analysis.csv', index=False)\n",
    "print(\"‚úì Feature importance analysis saved to: results/feature_importance_analysis.csv\")\n",
    "\n",
    "# Save selected features\n",
    "pd.Series(selected_features).to_csv('results/selected_features.csv', index=False, header=['feature'])\n",
    "print(\"‚úì Selected features saved to: results/selected_features.csv\")\n",
    "\n",
    "# Save best hyperparameters\n",
    "hyperparams = {\n",
    "    'LightGBM': lgb_best_params,\n",
    "    'XGBoost': xgb_best_params,\n",
    "    'CatBoost': cb_best_params,\n",
    "    'RandomForest': rf_random.best_params_,\n",
    "    'ExtraTrees': et_random.best_params_,\n",
    "    'Ridge': {'alpha': ridge_grid.best_params_['alpha']},\n",
    "    'Lasso': {'alpha': lasso_grid.best_params_['alpha']},\n",
    "    'ElasticNet': elastic_grid.best_params_\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('results/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(hyperparams, f, indent=2)\n",
    "print(\"‚úì Best hyperparameters saved to: results/best_hyperparameters.json\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODELING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nüìà FINAL SUMMARY:\")\n",
    "print(f\"   Models Evaluated: {len(evaluator.results)}\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   Best RMSE: {best_model_rmse:.4f}\")\n",
    "print(f\"   Best R¬≤: {best_model_r2:.4f}\")\n",
    "print(f\"   Features Used: {len(selected_features)}\")\n",
    "print(f\"   Cross-Validation: {evaluator.cv_folds}-fold\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
    "print(f\"   1. Use {best_model_name} for final predictions\")\n",
    "print(f\"   2. Consider ensemble if marginal improvement needed\")\n",
    "print(f\"   3. Monitor for overfitting with {len(selected_features)} features\")\n",
    "print(f\"   4. Validate on holdout test set before deployment\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
