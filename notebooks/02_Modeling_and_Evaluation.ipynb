{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BigMart Sales Prediction - Comprehensive Modeling Pipeline\n",
    "# Colab-Ready Version with Progressive Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSTALLATION AND IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install lightgbm xgboost catboost optuna scikit-learn --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Linear Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Tree-based Models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "# Advanced Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"‚úì All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATA LOADING AND PREPARATION - FIXED FOR COLAB\n",
    "# =============================================================================\n",
    "\n",
    "def load_processed_data():\n",
    "    \"\"\"Load preprocessed data - Colab version\"\"\"\n",
    "    print(\"Loading processed data...\")\n",
    "    \n",
    "    try:\n",
    "        # First try to load from previous notebook output\n",
    "        train_df = pd.read_csv('train_processed.csv')\n",
    "        print(f\"‚úì Loaded processed training data: {train_df.shape}\")\n",
    "        return train_df\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå train_processed.csv not found!\")\n",
    "        print(\"Please run the first notebook (EDA & Feature Engineering) first\")\n",
    "        print(\"Or upload your processed data file\")\n",
    "        return None\n",
    "\n",
    "def prepare_modeling_data(df, target_col='Item_Outlet_Sales'):\n",
    "    \"\"\"Prepare data for modeling - FIXED VERSION\"\"\"\n",
    "    if df is None:\n",
    "        return None, None, None\n",
    "        \n",
    "    print(\"Preparing data for modeling...\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    exclude_cols = ['Item_Identifier', 'Outlet_Identifier', target_col]\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    print(\"Converting categorical columns to numeric...\")\n",
    "    categorical_converted = 0\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "            try:\n",
    "                X[col] = pd.Categorical(X[col]).codes\n",
    "                categorical_converted += 1\n",
    "                print(f\"   - Converted {col} to numeric codes\")\n",
    "            except Exception as e:\n",
    "                print(f\"   - Warning: Could not convert {col}: {e}\")\n",
    "                # If conversion fails, drop the column\n",
    "                X = X.drop(col, axis=1)\n",
    "                feature_cols.remove(col)\n",
    "    \n",
    "    print(f\"   ‚úì Converted {categorical_converted} categorical columns to numeric\")\n",
    "    \n",
    "    # Verify all columns are numeric\n",
    "    non_numeric_cols = []\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype not in ['int64', 'float64', 'int32', 'float32']:\n",
    "            non_numeric_cols.append(col)\n",
    "    \n",
    "    if non_numeric_cols:\n",
    "        print(f\"   - Dropping non-numeric columns: {non_numeric_cols}\")\n",
    "        X = X.drop(non_numeric_cols, axis=1)\n",
    "        feature_cols = [col for col in feature_cols if col not in non_numeric_cols]\n",
    "    \n",
    "    print(f\"‚úì Features: {len(feature_cols)}\")\n",
    "    print(f\"‚úì Target variable: {target_col}\")\n",
    "    print(f\"‚úì Sample size: {len(X)}\")\n",
    "    print(f\"‚úì All features are now numeric\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Load data\n",
    "train_df = load_processed_data()\n",
    "\n",
    "if train_df is not None:\n",
    "    X, y, feature_cols = prepare_modeling_data(train_df)\n",
    "    print(f\"\\nTarget variable statistics:\")\n",
    "    print(y.describe())\n",
    "else:\n",
    "    print(\"Cannot proceed without processed data. Please run EDA notebook first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. FEATURE SELECTION \n",
    "# =============================================================================\n",
    "\n",
    "def perform_feature_selection(X, y, k=15):\n",
    "    \"\"\"Perform statistical feature selection\"\"\"\n",
    "    if X is None or y is None:\n",
    "        return None, None, None\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FEATURE SELECTION (SelectKBest, k={k})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Apply SelectKBest\n",
    "    selector = SelectKBest(score_func=f_regression, k=min(k, X.shape[1]))\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    feature_scores = selector.scores_[selector.get_support()]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'F_Score': feature_scores\n",
    "    }).sort_values('F_Score', ascending=False)\n",
    "    \n",
    "    print(\"Selected Features (ranked by F-score):\")\n",
    "    print(feature_importance_df.to_string(index=False, float_format='%.2f'))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['F_Score'])\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'])\n",
    "    plt.xlabel('F-Score')\n",
    "    plt.title(f'Top {len(selected_features)} Features by F-Score')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X_selected, selector, selected_features\n",
    "\n",
    "# Perform feature selection\n",
    "if 'X' in locals() and X is not None:\n",
    "    X_selected, feature_selector, selected_features = perform_feature_selection(X, y, k=15)\n",
    "    print(f\"\\n‚úì Selected features shape: {X_selected.shape if X_selected is not None else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. MODEL EVALUATION FRAMEWORK \n",
    "# =============================================================================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation framework\"\"\"\n",
    "    \n",
    "    def __init__(self, cv_folds=5, random_state=42):\n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, model, X, y, model_name, use_scaling=False):\n",
    "        \"\"\"Evaluate model using cross-validation\"\"\"\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_eval = X.copy()\n",
    "        if use_scaling:\n",
    "            scaler = StandardScaler()\n",
    "            X_eval = scaler.fit_transform(X_eval)\n",
    "        \n",
    "        # Cross-validation setup\n",
    "        kfold = KFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # Store fold results\n",
    "        rmse_scores = []\n",
    "        mae_scores = []\n",
    "        r2_scores = []\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_eval), 1):\n",
    "            X_train_fold = X_eval[train_idx]\n",
    "            X_val_fold = X_eval[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n",
    "            y_val_fold = y.iloc[val_idx] if hasattr(y, 'iloc') else y[val_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "            mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "            r2 = r2_score(y_val_fold, y_pred)\n",
    "            \n",
    "            rmse_scores.append(rmse)\n",
    "            mae_scores.append(mae)\n",
    "            r2_scores.append(r2)\n",
    "            \n",
    "            print(f\"  Fold {fold}: RMSE={rmse:.2f}, MAE={mae:.2f}, R¬≤={r2:.4f}\")\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'rmse_mean': np.mean(rmse_scores),\n",
    "            'rmse_std': np.std(rmse_scores),\n",
    "            'mae_mean': np.mean(mae_scores),\n",
    "            'mae_std': np.std(mae_scores),\n",
    "            'r2_mean': np.mean(r2_scores),\n",
    "            'r2_std': np.std(r2_scores),\n",
    "            'use_scaling': use_scaling\n",
    "        }\n",
    "        \n",
    "        self.results[model_name] = results\n",
    "        \n",
    "        print(f\"  Summary: RMSE={results['rmse_mean']:.2f}¬±{results['rmse_std']:.2f}, \"\n",
    "              f\"R¬≤={results['r2_mean']:.4f}¬±{results['r2_std']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_results_summary(self):\n",
    "        \"\"\"Get summary of all model results\"\"\"\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        results_df = pd.DataFrame(self.results).T\n",
    "        results_df = results_df.sort_values('rmse_mean')\n",
    "        \n",
    "        # Format results for display\n",
    "        results_df['CV_RMSE'] = (results_df['rmse_mean'].round(2).astype(str) + \n",
    "                                ' ¬± ' + results_df['rmse_std'].round(2).astype(str))\n",
    "        results_df['CV_R2'] = (results_df['r2_mean'].round(4).astype(str) + \n",
    "                              ' ¬± ' + results_df['r2_std'].round(4).astype(str))\n",
    "        \n",
    "        return results_df[['CV_RMSE', 'CV_R2', 'use_scaling']]\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot comparison of model results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to plot!\")\n",
    "            return\n",
    "        \n",
    "        results_df = pd.DataFrame(self.results).T\n",
    "        results_df = results_df.sort_values('rmse_mean')\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # RMSE comparison\n",
    "        axes[0].barh(range(len(results_df)), results_df['rmse_mean'])\n",
    "        axes[0].set_yticks(range(len(results_df)))\n",
    "        axes[0].set_yticklabels(results_df.index)\n",
    "        axes[0].set_xlabel('RMSE')\n",
    "        axes[0].set_title('Model Comparison - RMSE (Lower is Better)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # R¬≤ comparison\n",
    "        axes[1].barh(range(len(results_df)), results_df['r2_mean'])\n",
    "        axes[1].set_yticks(range(len(results_df)))\n",
    "        axes[1].set_yticklabels(results_df.index)\n",
    "        axes[1].set_xlabel('R¬≤ Score')\n",
    "        axes[1].set_title('Model Comparison - R¬≤ (Higher is Better)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(cv_folds=5)\n",
    "print(\"‚úì Model evaluator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. PHASE 1: BASELINE LINEAR MODELS\n",
    "# =============================================================================\n",
    "\n",
    "if 'X_selected' in locals() and X_selected is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE 1: BASELINE LINEAR MODELS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Multiple Linear Regression\n",
    "    print(\"\\n1.  Multiple Linear Regression\")\n",
    "    lr_model = LinearRegression()\n",
    "    evaluator.evaluate_model(lr_model, X_selected, y, \"Linear_Regression\", use_scaling=True)\n",
    "    \n",
    "    # 2. Ridge Regression with hyperparameter tuning\n",
    "    print(\"\\n2.  Ridge Regression (with hyperparameter tuning)\")\n",
    "    ridge_params = {'alpha': [0.1, 1.0, 10.0, 50.0, 100.0, 200.0, 500.0]}\n",
    "    ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=3, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Scale data for grid search\n",
    "    X_scaled = StandardScaler().fit_transform(X_selected)\n",
    "    ridge_grid.fit(X_scaled, y)\n",
    "    \n",
    "    print(f\"   ‚úì Best Ridge alpha: {ridge_grid.best_params_['alpha']}\")\n",
    "    ridge_model = Ridge(alpha=ridge_grid.best_params_['alpha'])\n",
    "    evaluator.evaluate_model(ridge_model, X_selected, y, \"Ridge_Regression\", use_scaling=True)\n",
    "    \n",
    "    # 3. Lasso Regression with hyperparameter tuning\n",
    "    print(\"\\n3.  Lasso Regression (with hyperparameter tuning)\")\n",
    "    lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 50.0, 100.0]}\n",
    "    lasso_grid = GridSearchCV(Lasso(), lasso_params, cv=3, scoring='neg_mean_squared_error')\n",
    "    lasso_grid.fit(X_scaled, y)\n",
    "    \n",
    "    print(f\"   ‚úì Best Lasso alpha: {lasso_grid.best_params_['alpha']}\")\n",
    "    lasso_model = Lasso(alpha=lasso_grid.best_params_['alpha'])\n",
    "    evaluator.evaluate_model(lasso_model, X_selected, y, \"Lasso_Regression\", use_scaling=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping linear models - no processed data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 5. PHASE 2: TREE-BASED MODELS\n",
    "# =============================================================================\n",
    "\n",
    "if 'X_selected' in locals() and X_selected is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE 2: TREE-BASED MODELS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 4. Decision Tree\n",
    "    print(\"\\n4. üå≥ Decision Tree (with hyperparameter tuning)\")\n",
    "    dt_params = {\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    dt_grid = GridSearchCV(DecisionTreeRegressor(random_state=42), dt_params, \n",
    "                           cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    dt_grid.fit(X_selected, y)\n",
    "    \n",
    "    print(f\"   ‚úì Best Decision Tree params: {dt_grid.best_params_}\")\n",
    "    dt_model = DecisionTreeRegressor(**dt_grid.best_params_, random_state=42)\n",
    "    evaluator.evaluate_model(dt_model, X_selected, y, \"Decision_Tree\")\n",
    "    \n",
    "    # 5. Random Forest\n",
    "    print(\"\\n5. üå≤ Random Forest (with hyperparameter tuning)\")\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    rf_random = RandomizedSearchCV(RandomForestRegressor(random_state=42), rf_params,\n",
    "                                   n_iter=15, cv=3, scoring='neg_mean_squared_error', \n",
    "                                   n_jobs=-1, random_state=42)\n",
    "    rf_random.fit(X_selected, y)\n",
    "    \n",
    "    print(f\"   ‚úì Best Random Forest params: {rf_random.best_params_}\")\n",
    "    rf_model = RandomForestRegressor(**rf_random.best_params_, random_state=42)\n",
    "    evaluator.evaluate_model(rf_model, X_selected, y, \"Random_Forest\")\n",
    "    \n",
    "    # 6. Extra Trees\n",
    "    print(\"\\n6.  Extra Trees (with hyperparameter tuning)\")\n",
    "    et_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    et_random = RandomizedSearchCV(ExtraTreesRegressor(random_state=42), et_params,\n",
    "                                   n_iter=15, cv=3, scoring='neg_mean_squared_error',\n",
    "                                   n_jobs=-1, random_state=42)\n",
    "    et_random.fit(X_selected, y)\n",
    "    \n",
    "    print(f\"   ‚úì Best Extra Trees params: {et_random.best_params_}\")\n",
    "    et_model = ExtraTreesRegressor(**et_random.best_params_, random_state=42)\n",
    "    evaluator.evaluate_model(et_model, X_selected, y, \"Extra_Trees\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping tree-based models - no processed data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. PHASE 3: ADVANCED ENSEMBLE MODELS WITH OPTUNA OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "if 'X_selected' in locals() and X_selected is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE 3: ADVANCED ENSEMBLE MODELS WITH OPTUNA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 7. LightGBM with Optuna optimization\n",
    "    print(\"\\n7. LightGBM (Optuna Optimization)\")\n",
    "    \n",
    "    def optimize_lightgbm(X, y, n_trials=200):  \n",
    "        \"\"\"Optimize LightGBM with Optuna\"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                'random_state': 42,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            \n",
    "            # Quick cross-validation for optimization\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            rmse_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in kfold.split(X):\n",
    "                X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = model.predict(X_val_fold)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "                rmse_scores.append(rmse)\n",
    "            \n",
    "            return np.mean(rmse_scores)\n",
    "        \n",
    "        print(f\"   Starting Optuna optimization with {n_trials} trials...\")\n",
    "        study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"   ‚úì Best RMSE: {study.best_value:.4f}\")\n",
    "        print(f\"   ‚úì Best params: {study.best_params}\")\n",
    "        return study.best_params\n",
    "    \n",
    "    # Run LightGBM optimization\n",
    "    lgb_best_params = optimize_lightgbm(X_selected, y, n_trials=200)  # ‚Üê CHANGE THIS NUMBER\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_best_params)\n",
    "    evaluator.evaluate_model(lgb_model, X_selected, y, \"LightGBM_Optuna\")\n",
    "    \n",
    "    # 8. XGBoost with Optuna optimization\n",
    "    print(\"\\n8. XGBoost (Optuna Optimization)\")\n",
    "    \n",
    "    def optimize_xgboost(X, y, n_trials=200):  \n",
    "        \"\"\"Optimize XGBoost with Optuna\"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "                'random_state': 42,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            \n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            rmse_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in kfold.split(X):\n",
    "                X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = model.predict(X_val_fold)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "                rmse_scores.append(rmse)\n",
    "            \n",
    "            return np.mean(rmse_scores)\n",
    "        \n",
    "        print(f\"   Starting Optuna optimization with {n_trials} trials...\")\n",
    "        study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"   ‚úì Best RMSE: {study.best_value:.4f}\")\n",
    "        print(f\"   ‚úì Best params: {study.best_params}\")\n",
    "        return study.best_params\n",
    "    \n",
    "    # Run XGBoost optimization\n",
    "    xgb_best_params = optimize_xgboost(X_selected, y, n_trials=200)  \n",
    "    xgb_model = xgb.XGBRegressor(**xgb_best_params)\n",
    "    evaluator.evaluate_model(xgb_model, X_selected, y, \"XGBoost_Optuna\")\n",
    "    \n",
    "    # 9. CatBoost with Optuna optimization\n",
    "    print(\"\\n9.  CatBoost (Optuna Optimization)\")\n",
    "    \n",
    "    def optimize_catboost(X, y, n_trials=200):  # ‚Üê SET TRIALS HERE\n",
    "        \"\"\"Optimize CatBoost with Optuna\"\"\"\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 500, 2000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'depth': trial.suggest_int('depth', 3, 10),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "                'random_state': 42,\n",
    "                'verbose': False\n",
    "            }\n",
    "            \n",
    "            model = cb.CatBoostRegressor(**params)\n",
    "            kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            rmse_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in kfold.split(X):\n",
    "                X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = model.predict(X_val_fold)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "                rmse_scores.append(rmse)\n",
    "            \n",
    "            return np.mean(rmse_scores)\n",
    "        \n",
    "        print(f\"   Starting Optuna optimization with {n_trials} trials...\")\n",
    "        study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        print(f\"   ‚úì Best RMSE: {study.best_value:.4f}\")\n",
    "        print(f\"   ‚úì Best params: {study.best_params}\")\n",
    "        return study.best_params\n",
    "    \n",
    "    # Run CatBoost optimization\n",
    "    cb_best_params = optimize_catboost(X_selected, y, n_trials=200)  # ‚Üê CHANGE THIS NUMBER\n",
    "    cb_model = cb.CatBoostRegressor(**cb_best_params)\n",
    "    evaluator.evaluate_model(cb_model, X_selected, y, \"CatBoost_Optuna\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping Optuna optimization - no processed data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. MODEL COMPARISON AND RESULTS ANALYSIS \n",
    "# =============================================================================\n",
    "\n",
    "if evaluator.results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MODEL COMPARISON AND RESULTS ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Display results summary\n",
    "    results_summary = evaluator.get_results_summary()\n",
    "    print(\"\\n Model Performance Summary (sorted by RMSE):\")\n",
    "    print(results_summary.to_string())\n",
    "    \n",
    "    # Plot model comparison\n",
    "    evaluator.plot_results()\n",
    "    \n",
    "    results_df = pd.DataFrame(evaluator.results).T\n",
    "    \n",
    "    # Convert key metrics to numeric, handling any string values\n",
    "    numeric_cols = ['rmse_mean', 'rmse_std', 'mae_mean', 'mae_std', 'r2_mean', 'r2_std']\n",
    "    for col in numeric_cols:\n",
    "        if col in results_df.columns:\n",
    "            results_df[col] = pd.to_numeric(results_df[col], errors='coerce')\n",
    "    \n",
    "    # Remove any rows with NaN in critical columns\n",
    "    results_df_clean = results_df.dropna(subset=['rmse_mean'])\n",
    "    \n",
    "    # Now safe to use nsmallest\n",
    "    best_3_models = results_df_clean.nsmallest(3, 'rmse_mean')\n",
    "    \n",
    "    print(f\"\\n Top 3 Best Performing Models:\")\n",
    "    for i, (model_name, row) in enumerate(best_3_models.iterrows(), 1):\n",
    "        rmse_std = row.get('rmse_std', float('nan'))\n",
    "        r2_mean = row.get('r2_mean', float('nan'))\n",
    "        r2_std = row.get('r2_std', float('nan'))\n",
    "        \n",
    "        rmse_str = f\"{rmse_std:.2f}\" if not pd.isna(rmse_std) else \"N/A\"\n",
    "        r2_mean_str = f\"{r2_mean:.4f}\" if not pd.isna(r2_mean) else \"N/A\"\n",
    "        r2_std_str = f\"{r2_std:.4f}\" if not pd.isna(r2_std) else \"N/A\"\n",
    "        \n",
    "        print(f\"   {i}. {model_name}: RMSE={row['rmse_mean']:.2f}¬±{rmse_str}, \"\n",
    "              f\"R¬≤={r2_mean_str}¬±{r2_std_str}\")\n",
    "    \n",
    "    # Best model selection \n",
    "    best_model_name = results_df_clean['rmse_mean'].idxmin()\n",
    "    best_model_rmse = results_df_clean.loc[best_model_name, 'rmse_mean']\n",
    "    best_model_r2 = results_df_clean.loc[best_model_name, 'r2_mean']\n",
    "    \n",
    "    print(f\"\\n BEST MODEL: {best_model_name}\")\n",
    "    print(f\"   RMSE: {best_model_rmse:.2f}¬±{results_df_clean.loc[best_model_name, 'rmse_std']:.2f}\")\n",
    "    print(f\"   R¬≤: {best_model_r2:.4f}¬±{results_df_clean.loc[best_model_name, 'r2_std']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No model results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if 'selected_features' in locals() and evaluator.results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get feature importance from tree-based models\n",
    "    importance_data = {}\n",
    "    \n",
    "    # Train models on full data for feature importance\n",
    "    if 'rf_model' in locals():\n",
    "        rf_model.fit(X_selected, y)\n",
    "        importance_data['Random_Forest'] = rf_model.feature_importances_\n",
    "    \n",
    "    if 'et_model' in locals():\n",
    "        et_model.fit(X_selected, y)\n",
    "        importance_data['Extra_Trees'] = et_model.feature_importances_\n",
    "    \n",
    "    if 'lgb_model' in locals():\n",
    "        lgb_model.fit(X_selected, y)\n",
    "        importance_data['LightGBM'] = lgb_model.feature_importances_\n",
    "    \n",
    "    if 'xgb_model' in locals():\n",
    "        xgb_model.fit(X_selected, y)\n",
    "        importance_data['XGBoost'] = xgb_model.feature_importances_\n",
    "    \n",
    "    if 'cb_model' in locals():\n",
    "        cb_model.fit(X_selected, y)\n",
    "        importance_data['CatBoost'] = cb_model.feature_importances_\n",
    "    \n",
    "    if importance_data:\n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame(importance_data, index=selected_features)\n",
    "        importance_df['Average'] = importance_df.mean(axis=1)\n",
    "        importance_df = importance_df.sort_values('Average', ascending=False)\n",
    "        \n",
    "        print(\"üîç Feature Importance Summary (Top 10):\")\n",
    "        print(importance_df.head(10).round(4).to_string())\n",
    "        \n",
    "        # Visualize average feature importance\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        top_10 = importance_df.head(10)\n",
    "        plt.barh(range(len(top_10)), top_10['Average'])\n",
    "        plt.yticks(range(len(top_10)), top_10.index)\n",
    "        plt.xlabel('Average Feature Importance')\n",
    "        plt.title('Top 10 Features by Average Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "if evaluator.results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Save results summary\n",
    "        results_summary = evaluator.get_results_summary()\n",
    "        results_summary.to_csv('model_performance_summary.csv')\n",
    "        print(\"‚úì Model performance summary saved: model_performance_summary.csv\")\n",
    "        \n",
    "        # Save selected features\n",
    "        if 'selected_features' in locals():\n",
    "            pd.Series(selected_features).to_csv('selected_features.csv', \n",
    "                                               index=False, header=['feature'])\n",
    "            print(\"‚úì Selected features saved: selected_features.csv\")\n",
    "        \n",
    "        # Save feature importance if available\n",
    "        if 'importance_df' in locals():\n",
    "            importance_df.to_csv('feature_importance_analysis.csv')\n",
    "            print(\"‚úì Feature importance saved: feature_importance_analysis.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving files: {e}\")\n",
    "        print(\"You can still access results from the notebook variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODELING PIPELINE COMPLETED!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if evaluator.results:\n",
    "    print(f\"\\n FINAL SUMMARY:\")\n",
    "    print(f\"    Models Evaluated: {len(evaluator.results)}\")\n",
    "    if 'best_model_name' in locals():\n",
    "        print(f\"   üèÜ Best Model: {best_model_name}\")\n",
    "        print(f\"   üìà Best RMSE: {best_model_rmse:.2f}\")\n",
    "        print(f\"   üìä Best R¬≤: {best_model_r2:.4f}\")\n",
    "    if 'selected_features' in locals():\n",
    "        print(f\"   üéØ Features Used: {len(selected_features)}\")\n",
    "    print(f\"   üîÑ Cross-Validation: 5-fold\")\n",
    "    \n",
    "    print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
    "    if 'best_model_name' in locals():\n",
    "        print(f\"   1. Use {best_model_name} for final predictions\")\n",
    "        print(f\"   2. Consider ensemble if marginal improvement needed\")\n",
    "        print(f\"   3. Validate on holdout test set before deployment\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models were successfully evaluated.\")\n",
    "    print(\"Please check the data loading and preprocessing steps.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
