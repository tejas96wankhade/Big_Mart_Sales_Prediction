{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# BigMart Sales Prediction - Exploratory Data Analysis & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set style for better visualizations - FIXED\n",
    "plt.style.use('default')  # Use default instead of seaborn-v0_8\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. DATA LOADING AND INITIAL EXPLORATION - FIXED\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(train_path='train.csv', test_path='test.csv'):  \n",
    "    \"\"\"Load training and test datasets\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        print(\"Dataset Shape:\")\n",
    "        print(f\"Training data: {train_df.shape}\")\n",
    "        print(f\"Test data: {test_df.shape}\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please upload your train.csv and test.csv files to Colab first!\")\n",
    "        return None, None\n",
    "\n",
    "def basic_info(df, name=\"Dataset\"):\n",
    "    \"\"\"Display basic information about the dataset\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name.upper()} BASIC INFORMATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(f\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "def check_duplicates(df, name=\"Dataset\"):\n",
    "    \"\"\"Check for duplicate records\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{name} - Duplicate Records:\")\n",
    "    print(f\"Total duplicates: {df.duplicated().sum()}\")\n",
    "    if df.duplicated().sum() > 0:\n",
    "        print(\"Duplicate indices:\")\n",
    "        print(df[df.duplicated()].index.tolist())\n",
    "\n",
    "# Load data \n",
    "train_df, test_df = load_data()\n",
    "\n",
    "if train_df is not None and test_df is not None:\n",
    "    # Basic exploration\n",
    "    basic_info(train_df, \"Training Data\")\n",
    "    basic_info(test_df, \"Test Data\")\n",
    "    \n",
    "    # Check duplicates\n",
    "    check_duplicates(train_df, \"Training\")\n",
    "    check_duplicates(test_df, \"Test\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows of training data:\")\n",
    "    print(train_df.head())\n",
    "else:\n",
    "    print(\"Cannot proceed without data files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 2. MISSING VALUES ANALYSIS \n",
    "# =============================================================================\n",
    "\n",
    "def analyze_missing_values(df, name=\"Dataset\"):\n",
    "    \"\"\"Comprehensive missing values analysis\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name.upper()} - MISSING VALUES ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing_Count': missing_data,\n",
    "        'Missing_Percentage': missing_percent\n",
    "    })\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df.to_string(index=False))\n",
    "        \n",
    "        # Visualize missing values\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        missing_df.plot(x='Column', y='Missing_Count', kind='bar', ax=axes[0])\n",
    "        axes[0].set_title(f'{name} - Missing Values Count')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        missing_df.plot(x='Column', y='Missing_Percentage', kind='bar', ax=axes[1], color='orange')\n",
    "        axes[1].set_title(f'{name} - Missing Values Percentage')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "\n",
    "if train_df is not None:\n",
    "    # Analyze missing values in both datasets\n",
    "    analyze_missing_values(train_df, \"Training\")\n",
    "    analyze_missing_values(test_df, \"Test\")\n",
    "    \n",
    "    # Combine datasets for comprehensive missing value analysis\n",
    "    combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    analyze_missing_values(combined_df, \"Combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. TARGET VARIABLE ANALYSIS \n",
    "# =============================================================================\n",
    "\n",
    "def analyze_target_variable(df, target_col='Item_Outlet_Sales'):\n",
    "    \"\"\"Comprehensive target variable analysis\"\"\"\n",
    "    if df is None or target_col not in df.columns:\n",
    "        print(f\"Target column {target_col} not found!\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"TARGET VARIABLE ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    target = df[target_col]\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"Basic Statistics:\")\n",
    "    print(target.describe())\n",
    "    \n",
    "    print(f\"\\nAdditional Statistics:\")\n",
    "    print(f\"Skewness: {target.skew():.4f}\")\n",
    "    print(f\"Kurtosis: {target.kurtosis():.4f}\")\n",
    "    print(f\"Min value: {target.min():.2f}\")\n",
    "    print(f\"Max value: {target.max():.2f}\")\n",
    "    print(f\"Zero values: {(target == 0).sum()}\")\n",
    "    print(f\"Negative values: {(target < 0).sum()}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0, 0].hist(target, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribution of Item_Outlet_Sales')\n",
    "    axes[0, 0].set_xlabel('Sales')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0, 1].boxplot(target)\n",
    "    axes[0, 1].set_title('Box Plot of Item_Outlet_Sales')\n",
    "    axes[0, 1].set_ylabel('Sales')\n",
    "    \n",
    "    # Q-Q plot for normality check\n",
    "    stats.probplot(target, dist=\"norm\", plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('Q-Q Plot (Normal Distribution)')\n",
    "    \n",
    "    # Log transformation\n",
    "    log_target = np.log1p(target)  # log1p handles zeros better\n",
    "    axes[1, 1].hist(log_target, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1, 1].set_title('Log-transformed Distribution')\n",
    "    axes[1, 1].set_xlabel('Log(Sales + 1)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if train_df is not None:\n",
    "    # Analyze target variable\n",
    "    analyze_target_variable(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. FEATURE ENGINEERING PIPELINE \n",
    "# =============================================================================\n",
    "\n",
    "class FeatureEngineeringPipeline:\n",
    "    \"\"\"Comprehensive feature engineering pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.mean_values = {}\n",
    "        self.mode_values = {}\n",
    "    \n",
    "    def fit_transform(self, df, target_col=None):\n",
    "        \"\"\"Apply all feature engineering transformations\"\"\"\n",
    "        if df is None:\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"FEATURE ENGINEERING PIPELINE\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # 1. Handle missing values\n",
    "        print(\"1. Handling missing values...\")\n",
    "        df_processed = self._handle_missing_values(df_processed)\n",
    "        \n",
    "        # 2. Standardize categorical values\n",
    "        print(\"2. Standardizing categorical values...\")\n",
    "        df_processed = self._standardize_categories(df_processed)\n",
    "        \n",
    "        # 3. Handle zero visibility\n",
    "        print(\"3. Handling zero visibility values...\")\n",
    "        df_processed = self._handle_zero_visibility(df_processed)\n",
    "        \n",
    "        # 4. Create new features\n",
    "        print(\"4. Creating engineered features...\")\n",
    "        df_processed = self._create_engineered_features(df_processed)\n",
    "        \n",
    "        # 5. Encode categorical variables\n",
    "        print(\"5. Encoding categorical variables...\")\n",
    "        df_processed = self._encode_categorical_features(df_processed)\n",
    "        \n",
    "        print(\"Feature engineering completed!\")\n",
    "        print(f\"Original features: {df.shape[1]}\")\n",
    "        print(f\"Final features: {df_processed.shape[1]}\")\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def _handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values\"\"\"\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Item_Weight: Fill with mean\n",
    "        if 'Item_Weight' in df_clean.columns:\n",
    "            if 'Item_Weight' not in self.mean_values:\n",
    "                self.mean_values['Item_Weight'] = df_clean['Item_Weight'].mean()\n",
    "            mean_weight = self.mean_values['Item_Weight']\n",
    "            missing_count = df['Item_Weight'].isnull().sum()\n",
    "            df_clean['Item_Weight'].fillna(mean_weight, inplace=True)\n",
    "            if missing_count > 0:\n",
    "                print(f\"   - Item_Weight: Filled {missing_count} missing values with mean ({mean_weight:.3f})\")\n",
    "        \n",
    "        # Outlet_Size: Fill with mode\n",
    "        if 'Outlet_Size' in df_clean.columns:\n",
    "            if 'Outlet_Size' not in self.mode_values:\n",
    "                self.mode_values['Outlet_Size'] = df_clean['Outlet_Size'].mode()[0] if len(df_clean['Outlet_Size'].mode()) > 0 else 'Medium'\n",
    "            mode_size = self.mode_values['Outlet_Size']\n",
    "            missing_count = df['Outlet_Size'].isnull().sum()\n",
    "            df_clean['Outlet_Size'].fillna(mode_size, inplace=True)\n",
    "            if missing_count > 0:\n",
    "                print(f\"   - Outlet_Size: Filled {missing_count} missing values with mode ({mode_size})\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def _standardize_categories(self, df):\n",
    "        \"\"\"Standardize categorical values\"\"\"\n",
    "        df_std = df.copy()\n",
    "        \n",
    "        # Item_Fat_Content standardization\n",
    "        if 'Item_Fat_Content' in df_std.columns:\n",
    "            before_counts = df_std['Item_Fat_Content'].value_counts()\n",
    "            \n",
    "            df_std['Item_Fat_Content'] = df_std['Item_Fat_Content'].replace({\n",
    "                'low fat': 'Low Fat',\n",
    "                'LF': 'Low Fat', \n",
    "                'reg': 'Regular'\n",
    "            })\n",
    "            \n",
    "            after_counts = df_std['Item_Fat_Content'].value_counts()\n",
    "            print(f\"   - Item_Fat_Content standardized\")\n",
    "            print(f\"     Before: {dict(before_counts)}\")\n",
    "            print(f\"     After: {dict(after_counts)}\")\n",
    "        \n",
    "        return df_std\n",
    "    \n",
    "    def _handle_zero_visibility(self, df):\n",
    "        \"\"\"Handle zero visibility values\"\"\"\n",
    "        df_vis = df.copy()\n",
    "        \n",
    "        if 'Item_Visibility' in df_vis.columns:\n",
    "            zero_count = (df_vis['Item_Visibility'] == 0).sum()\n",
    "            if zero_count > 0:\n",
    "                mean_visibility = df_vis[df_vis['Item_Visibility'] > 0]['Item_Visibility'].mean()\n",
    "                df_vis['Item_Visibility'] = df_vis['Item_Visibility'].replace(0, mean_visibility)\n",
    "                print(f\"   - Item_Visibility: Replaced {zero_count} zero values with mean ({mean_visibility:.6f})\")\n",
    "        \n",
    "        return df_vis\n",
    "    \n",
    "    def _create_engineered_features(self, df):\n",
    "        \"\"\"Create new engineered features\"\"\"\n",
    "        df_eng = df.copy()\n",
    "        \n",
    "        # 1. Outlet Age\n",
    "        if 'Outlet_Establishment_Year' in df_eng.columns:\n",
    "            df_eng['Outlet_Age'] = 2013 - df_eng['Outlet_Establishment_Year']\n",
    "            print(f\"   - Created Outlet_Age feature (range: {df_eng['Outlet_Age'].min()}-{df_eng['Outlet_Age'].max()})\")\n",
    "        \n",
    "        # 2. Item Visibility features - \n",
    "        if 'Item_Visibility' in df_eng.columns and 'Outlet_Identifier' in df_eng.columns:\n",
    "            # Mean visibility per outlet\n",
    "            outlet_visibility_mean = df_eng.groupby('Outlet_Identifier')['Item_Visibility'].transform('mean')\n",
    "            df_eng['Item_Visibility_Outlet_Mean'] = outlet_visibility_mean\n",
    "            \n",
    "            # Visibility ratio - : Better handling of division\n",
    "            df_eng['Item_Visibility_Ratio'] = df_eng['Item_Visibility'] / (df_eng['Item_Visibility_Outlet_Mean'] + 1e-8)\n",
    "            print(f\"   - Created visibility features: Item_Visibility_Outlet_Mean, Item_Visibility_Ratio\")\n",
    "        \n",
    "        # 3. Item MRP Category\n",
    "        if 'Item_MRP' in df_eng.columns:\n",
    "            df_eng['Item_MRP_Category'] = pd.cut(df_eng['Item_MRP'], \n",
    "                                               bins=[0, 69, 136, 203, 270], \n",
    "                                               labels=[0, 1, 2, 3])\n",
    "            # Convert to numeric to avoid issues\n",
    "            df_eng['Item_MRP_Category'] = df_eng['Item_MRP_Category'].astype(int)\n",
    "            print(f\"   - Created Item_MRP_Category (4 categories based on price ranges)\")\n",
    "        \n",
    "        # 4. Interaction features - \n",
    "        if 'Item_MRP' in df_eng.columns and 'Item_Visibility' in df_eng.columns:\n",
    "            df_eng['Item_MRP_Visibility'] = df_eng['Item_MRP'] * df_eng['Item_Visibility']\n",
    "            print(f\"   - Created Item_MRP_Visibility interaction\")\n",
    "        \n",
    "        if 'Item_Weight' in df_eng.columns and 'Item_Visibility' in df_eng.columns:\n",
    "            df_eng['Weight_Visibility'] = df_eng['Item_Weight'] * df_eng['Item_Visibility']\n",
    "            print(f\"   - Created Weight_Visibility interaction\")\n",
    "        \n",
    "        if 'Item_MRP' in df_eng.columns and 'Item_Weight' in df_eng.columns:\n",
    "            df_eng['MRP_Weight_Ratio'] = df_eng['Item_MRP'] / (df_eng['Item_Weight'] + 1e-8)\n",
    "            print(f\"   - Created MRP_Weight_Ratio\")\n",
    "        \n",
    "        return df_eng\n",
    "    \n",
    "    def _encode_categorical_features(self, df):\n",
    "        \"\"\"Encode categorical features\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier',\n",
    "                           'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in df_encoded.columns:\n",
    "                if col not in self.label_encoders:\n",
    "                    self.label_encoders[col] = LabelEncoder()\n",
    "                    df_encoded[col] = df_encoded[col].fillna('Missing')\n",
    "                    df_encoded[col] = self.label_encoders[col].fit_transform(df_encoded[col])\n",
    "                else:\n",
    "                    # Transform using existing encoder\n",
    "                    df_encoded[col] = df_encoded[col].fillna('Missing')\n",
    "                    # Handle unknown categories\n",
    "                    unique_values = set(df_encoded[col].unique())\n",
    "                    known_values = set(self.label_encoders[col].classes_)\n",
    "                    unknown_values = unique_values - known_values\n",
    "                    if unknown_values:\n",
    "                        print(f\"   - Warning: Unknown categories in {col}: {unknown_values}\")\n",
    "                        # Replace unknown with 'Missing'\n",
    "                        df_encoded[col] = df_encoded[col].replace(list(unknown_values), 'Missing')\n",
    "                    df_encoded[col] = self.label_encoders[col].transform(df_encoded[col])\n",
    "                \n",
    "                print(f\"   - Encoded {col} ({len(self.label_encoders[col].classes_)} unique values)\")\n",
    "        \n",
    "        return df_encoded\n",
    "\n",
    "# Apply feature engineering if data is available\n",
    "if train_df is not None:\n",
    "    fe_pipeline = FeatureEngineeringPipeline()\n",
    "    train_processed = fe_pipeline.fit_transform(train_df)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nOriginal training data shape: {train_df.shape}\")\n",
    "    print(f\"Processed training data shape: {train_processed.shape}\")\n",
    "    \n",
    "    print(f\"\\nProcessing completed successfully!\")\n",
    "    \n",
    "    # Save processed data \n",
    "    try:\n",
    "        train_processed.to_csv('train_processed.csv', index=False)\n",
    "        print(\"✓ Processed training data saved as: train_processed.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save file - {e}\")\n",
    "        print(\"You can download the processed data manually from the notebook variables.\")\n",
    "    \n",
    "    print(f\"\\nFinal processed dataset:\")\n",
    "    print(f\"Shape: {train_processed.shape}\")\n",
    "    print(f\"Sample of processed data:\")\n",
    "    print(train_processed.head())\n",
    "else:\n",
    "    print(\"Cannot proceed with feature engineering without data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. CATEGORICAL FEATURES ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_categorical_features(df, target_col='Item_Outlet_Sales'):\n",
    "    \"\"\"Analyze categorical features\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"Unique values: {df[col].nunique()}\")\n",
    "        print(f\"Value counts:\")\n",
    "        print(df[col].value_counts())\n",
    "        \n",
    "        # Check for inconsistent labeling\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        print(f\"Unique values list: {sorted(unique_values)}\")\n",
    "        \n",
    "        # Visualizations\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Value counts bar plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        value_counts = df[col].value_counts()\n",
    "        plt.bar(range(len(value_counts)), value_counts.values)\n",
    "        plt.xticks(range(len(value_counts)), value_counts.index, rotation=45)\n",
    "        plt.title(f'{col} - Value Counts')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Average target by category (if target available)\n",
    "        if target_col in df.columns:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            avg_target = df.groupby(col)[target_col].mean().sort_values(ascending=False)\n",
    "            plt.bar(range(len(avg_target)), avg_target.values)\n",
    "            plt.xticks(range(len(avg_target)), avg_target.index, rotation=45)\n",
    "            plt.title(f'Average {target_col} by {col}')\n",
    "            plt.ylabel(f'Average {target_col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if train_df is not None:\n",
    "    # Analyze categorical features\n",
    "    analyze_categorical_features(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 6. DATA QUALITY ISSUES IDENTIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "def identify_data_quality_issues(df):\n",
    "    \"\"\"Identify various data quality issues\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DATA QUALITY ISSUES\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Item_Fat_Content inconsistencies\n",
    "    if 'Item_Fat_Content' in df.columns:\n",
    "        fat_content_values = df['Item_Fat_Content'].value_counts()\n",
    "        print(\"Item_Fat_Content inconsistencies:\")\n",
    "        print(fat_content_values)\n",
    "        \n",
    "        inconsistent = ['low fat', 'LF', 'reg']\n",
    "        if any(val in fat_content_values.index for val in inconsistent):\n",
    "            issues.append(\"Item_Fat_Content has inconsistent labeling\")\n",
    "    \n",
    "    # 2. Item_Visibility zero values\n",
    "    if 'Item_Visibility' in df.columns:\n",
    "        zero_visibility = (df['Item_Visibility'] == 0).sum()\n",
    "        print(f\"\\nItem_Visibility zero values: {zero_visibility}\")\n",
    "        if zero_visibility > 0:\n",
    "            issues.append(\"Item_Visibility has impossible zero values\")\n",
    "    \n",
    "    # 3. Unusual patterns in other columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        if col != 'Item_Outlet_Sales':  # Skip target\n",
    "            # Check for exact zeros\n",
    "            zero_count = (df[col] == 0).sum()\n",
    "            if zero_count > 0 and col != 'Outlet_Establishment_Year':\n",
    "                print(f\"\\n{col} has {zero_count} zero values\")\n",
    "            \n",
    "            # Check for extreme outliers\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "            if outliers > 0:\n",
    "                print(f\"{col} has {outliers} potential outliers\")\n",
    "    \n",
    "    print(f\"\\nSummary of identified issues:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"{i}. {issue}\")\n",
    "\n",
    "if train_df is not None:\n",
    "    # Identify data quality issues\n",
    "    identify_data_quality_issues(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. FEATURE RELATIONSHIPS AND PATTERNS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_relationships(df, target_col='Item_Outlet_Sales'):\n",
    "    \"\"\"Analyze relationships between features\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"FEATURE RELATIONSHIPS ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Item_MRP vs Sales relationship\n",
    "    if 'Item_MRP' in df.columns and target_col in df.columns:\n",
    "        print(\"Item_MRP vs Item_Outlet_Sales relationship:\")\n",
    "        \n",
    "        # Create MRP categories\n",
    "        df['MRP_Category'] = pd.cut(df['Item_MRP'], \n",
    "                                   bins=[0, 69, 136, 203, 270], \n",
    "                                   labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "        \n",
    "        mrp_sales = df.groupby('MRP_Category')[target_col].agg(['mean', 'count'])\n",
    "        print(mrp_sales)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.scatter(df['Item_MRP'], df[target_col], alpha=0.6)\n",
    "        plt.xlabel('Item_MRP')\n",
    "        plt.ylabel(target_col)\n",
    "        plt.title('Item_MRP vs Sales')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        mrp_sales['mean'].plot(kind='bar')\n",
    "        plt.title('Average Sales by MRP Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        df.boxplot(column=target_col, by='MRP_Category', ax=plt.gca())\n",
    "        plt.title('Sales Distribution by MRP Category')\n",
    "        plt.suptitle('')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Outlet Type vs Sales\n",
    "    if 'Outlet_Type' in df.columns and target_col in df.columns:\n",
    "        print(\"\\nOutlet_Type vs Item_Outlet_Sales relationship:\")\n",
    "        \n",
    "        outlet_sales = df.groupby('Outlet_Type')[target_col].agg(['mean', 'count', 'std'])\n",
    "        print(outlet_sales)\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        outlet_sales['mean'].plot(kind='bar')\n",
    "        plt.title('Average Sales by Outlet Type')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        df.boxplot(column=target_col, by='Outlet_Type', ax=plt.gca())\n",
    "        plt.title('Sales Distribution by Outlet Type')\n",
    "        plt.suptitle('')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if train_df is not None:\n",
    "    # Analyze feature relationships\n",
    "    analyze_feature_relationships(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 8. APPLY COMPLETE FEATURE ENGINEERING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "# Apply feature engineering if data is available\n",
    "if train_df is not None:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"APPLYING COMPLETE FEATURE ENGINEERING PIPELINE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    fe_pipeline = FeatureEngineeringPipeline()\n",
    "    train_processed = fe_pipeline.fit_transform(train_df)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nOriginal training data shape: {train_df.shape}\")\n",
    "    print(f\"Processed training data shape: {train_processed.shape}\")\n",
    "    \n",
    "    print(f\"\\nNew feature columns:\")\n",
    "    original_cols = set(train_df.columns)\n",
    "    processed_cols = set(train_processed.columns)\n",
    "    new_cols = processed_cols - original_cols\n",
    "    for col in sorted(new_cols):\n",
    "        print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 9. FEATURE IMPORTANCE AND CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_importance(df, target_col='Item_Outlet_Sales'):\n",
    "    \"\"\"Analyze feature importance using correlation and statistical methods - FIXED\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    \n",
    "    df_numeric = df.copy()\n",
    "    \n",
    "    # Convert any categorical/object columns to numeric codes\n",
    "    categorical_cols = []\n",
    "    for col in df_numeric.columns:\n",
    "        if df_numeric[col].dtype == 'object' or df_numeric[col].dtype.name == 'category':\n",
    "            categorical_cols.append(col)\n",
    "            if col not in ['Item_Identifier', 'Outlet_Identifier']:  # Skip ID columns\n",
    "                try:\n",
    "                    df_numeric[col] = pd.Categorical(df_numeric[col]).codes\n",
    "                    print(f\"   - Converted {col} to numeric codes\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   - Warning: Could not convert {col}: {e}\")\n",
    "    \n",
    "    print(f\"   - Converted {len(categorical_cols)} categorical columns to numeric\")\n",
    "    \n",
    "    # Exclude non-numeric columns and identifiers\n",
    "    feature_cols = [col for col in df_numeric.columns \n",
    "                   if col not in ['Item_Identifier', 'Outlet_Identifier', target_col]]\n",
    "    \n",
    "    # Filter to only numeric columns\n",
    "    numeric_feature_cols = []\n",
    "    for col in feature_cols:\n",
    "        if df_numeric[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            numeric_feature_cols.append(col)\n",
    "    \n",
    "    print(f\"   - Analyzing {len(numeric_feature_cols)} numeric features\")\n",
    "    \n",
    "    X = df_numeric[numeric_feature_cols]\n",
    "    y = df_numeric[target_col]\n",
    "    \n",
    "    # Correlation with target\n",
    "    correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    print(\"\\nFeature correlations with target (absolute values):\")\n",
    "    print(correlations.head(15))\n",
    "    \n",
    "    # Visualize top correlations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_15_corr = correlations.head(15)\n",
    "    plt.barh(range(len(top_15_corr)), top_15_corr.values)\n",
    "    plt.yticks(range(len(top_15_corr)), top_15_corr.index)\n",
    "    plt.xlabel('Absolute Correlation with Target')\n",
    "    plt.title('Top 15 Features by Correlation with Target')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature correlation matrix (top features)\n",
    "    top_features = list(correlations.head(10).index) + [target_col]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = df_numeric[top_features].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "    plt.title('Correlation Matrix - Top Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "if 'train_processed' in locals():\n",
    "    # Analyze feature importance\n",
    "    feature_importance = analyze_feature_importance(train_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 10. VALIDATION AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def validate_preprocessing(original_df, processed_df):\n",
    "    \"\"\"Validate preprocessing results\"\"\"\n",
    "    if original_df is None or processed_df is None:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"PREPROCESSING VALIDATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    original_missing = original_df.isnull().sum().sum()\n",
    "    processed_missing = processed_df.isnull().sum().sum()\n",
    "    \n",
    "    print(f\"Missing values - Original: {original_missing}, Processed: {processed_missing}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    numeric_cols = processed_df.select_dtypes(include=[np.number]).columns\n",
    "    inf_values = np.isinf(processed_df[numeric_cols]).sum().sum()\n",
    "    print(f\"Infinite values in processed data: {inf_values}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\nData types after processing:\")\n",
    "    print(processed_df.dtypes.value_counts())\n",
    "    \n",
    "    # Verify no negative values where they shouldn't exist\n",
    "    negative_cols = []\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Item_Outlet_Sales':  # Target can be any value\n",
    "            negative_count = (processed_df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                negative_cols.append((col, negative_count))\n",
    "    \n",
    "    if negative_cols:\n",
    "        print(f\"\\nWarning - Negative values found:\")\n",
    "        for col, count in negative_cols:\n",
    "            print(f\"  {col}: {count} negative values\")\n",
    "    else:\n",
    "        print(f\"\\nNo unexpected negative values found ✓\")\n",
    "    \n",
    "    print(f\"\\n✓ Preprocessing validation completed\")\n",
    "\n",
    "if 'train_processed' in locals():\n",
    "    # Validate preprocessing\n",
    "    validate_preprocessing(train_df, train_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 11. COMPREHENSIVE SUMMARY AND DATA SAVING\n",
    "# =============================================================================\n",
    "\n",
    "if 'train_processed' in locals():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"1. Data Quality Improvements:\")\n",
    "    print(f\"   - Missing values handled: Item_Weight, Outlet_Size\")\n",
    "    print(f\"   - Inconsistent categories standardized: Item_Fat_Content\")\n",
    "    print(f\"   - Zero visibility values corrected: Item_Visibility\")\n",
    "    \n",
    "    print(f\"\\n2. New Features Created:\")\n",
    "    print(f\"   - Outlet_Age: Store maturity indicator\")\n",
    "    print(f\"   - Item_Visibility_Ratio: Relative visibility within outlet\")\n",
    "    print(f\"   - Item_MRP_Category: Price tier segmentation\")\n",
    "    print(f\"   - Interaction features: MRP×Visibility, Weight×Visibility, MRP/Weight\")\n",
    "    \n",
    "    print(f\"\\n3. Encoding Applied:\")\n",
    "    print(f\"   - Label encoding for all categorical variables\")\n",
    "    print(f\"   - Maintains ordinal relationships where applicable\")\n",
    "    \n",
    "    print(f\"\\n4. Feature Selection Candidates:\")\n",
    "    if 'feature_importance' in locals():\n",
    "        top_10_features = feature_importance.head(10)\n",
    "        print(f\"   Top 10 features by correlation:\")\n",
    "        for i, (feature, corr) in enumerate(top_10_features.items(), 1):\n",
    "            print(f\"   {i:2d}. {feature:<25} (r={corr:.4f})\")\n",
    "    \n",
    "    print(f\"\\n5. Recommendations for Modeling:\")\n",
    "    print(f\"   - Use top 15-20 features for initial modeling\")\n",
    "    print(f\"   - Consider feature scaling for linear models\")\n",
    "    print(f\"   - Tree-based models can handle current encoding\")\n",
    "    print(f\"   - Monitor for overfitting with engineered features\")\n",
    "    \n",
    "    # Save processed data\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING PROCESSED DATA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Save processed training data\n",
    "        train_processed.to_csv('train_processed.csv', index=False)\n",
    "        print(\"✓ Processed training data saved to: train_processed.csv\")\n",
    "        \n",
    "        # Display final shape and column info\n",
    "        print(f\"\\nFinal processed dataset:\")\n",
    "        print(f\"Shape: {train_processed.shape}\")\n",
    "        print(f\"Columns: {list(train_processed.columns)}\")\n",
    "        \n",
    "        # Save feature engineering summary\n",
    "        summary_info = {\n",
    "            'original_shape': train_df.shape,\n",
    "            'processed_shape': train_processed.shape,\n",
    "            'original_columns': list(train_df.columns),\n",
    "            'processed_columns': list(train_processed.columns),\n",
    "            'new_features': list(set(train_processed.columns) - set(train_df.columns)),\n",
    "            'preprocessing_steps': [\n",
    "                'Missing value imputation',\n",
    "                'Category standardization', \n",
    "                'Zero visibility correction',\n",
    "                'Feature engineering',\n",
    "                'Label encoding'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Save as text summary\n",
    "        with open('feature_engineering_summary.txt', 'w') as f:\n",
    "            f.write(\"BigMart Sales Prediction - Feature Engineering Summary\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            f.write(f\"Original dataset shape: {summary_info['original_shape']}\\n\")\n",
    "            f.write(f\"Processed dataset shape: {summary_info['processed_shape']}\\n\")\n",
    "            f.write(f\"Features added: {len(summary_info['new_features'])}\\n\\n\")\n",
    "            f.write(\"New Features Created:\\n\")\n",
    "            for feature in summary_info['new_features']:\n",
    "                f.write(f\"  - {feature}\\n\")\n",
    "            f.write(\"\\nPreprocessing Steps Applied:\\n\")\n",
    "            for step in summary_info['preprocessing_steps']:\n",
    "                f.write(f\"  - {step}\\n\")\n",
    "        \n",
    "        print(\" Feature engineering summary saved to: feature_engineering_summary.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error saving files: {e}\")\n",
    "        print(\"Files can be downloaded manually from notebook variables\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\" EDA AND FEATURE ENGINEERING COMPLETED!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n FINAL STATISTICS:\")\n",
    "    print(f\"   Original features: {len(train_df.columns)}\")\n",
    "    print(f\"   Final features: {len(train_processed.columns)}\")\n",
    "    print(f\"   New features created: {len(train_processed.columns) - len(train_df.columns)}\")\n",
    "    print(f\"   Data quality issues resolved: ✓\")\n",
    "    print(f\"   Missing values handled: ✓\")\n",
    "    print(f\"   Ready for modeling: ✓\")\n",
    "    \n",
    "else:\n",
    "    print(\"Feature engineering was not completed due to missing data\")\n",
    "    print(\"Please ensure your train.csv file is properly uploaded and accessible\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
